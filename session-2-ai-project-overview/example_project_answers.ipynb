{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import statsmodels.api as sm\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import plotly.figure_factory as ff\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from statsmodels.tools.tools import add_constant\n",
    "import seaborn as sns\n",
    "pio.renderers.default = 'iframe'"
   ],
   "id": "4e8abf172bba59ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 1: Load Data\n",
    "----"
   ],
   "id": "899bff3cfed98b8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data = pd.read_csv(\"datasets/orders.csv\")\n",
    "data.head()"
   ],
   "id": "e1ffbaea35ac04c6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 2: Check Data Characteristics\n",
    "---"
   ],
   "id": "240c722f17d9d365"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.shape",
   "id": "d47ee2f4eab84cd8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data.info()",
   "id": "835fd76e5289d33c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Missing values\n",
    "data.isna().sum()"
   ],
   "id": "ace24960ec45fc78"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 3: Data Preprocessing\n",
    "---"
   ],
   "id": "ed3bc0bbe05c6666"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data['order_id'] = data['order_id'] .astype(str)\n",
    "data['order_timestamp'] = pd.to_datetime(data['order_timestamp'])\n",
    "data['delivery_timestamp'] = pd.to_datetime(data['delivery_timestamp'])\n",
    "data['time_to_deliver'] = data.delivery_timestamp - data.order_timestamp"
   ],
   "id": "88ca240ccc948024"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Step 4: Perform Feature Engineering\n",
    "---\n",
    "1. Create a new feature called \"time to deliver\", the amount of time between order to delivery.\n",
    "2. Label Encode all categorical data."
   ],
   "id": "188f5c1f9f92f5d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "data2 = data.copy()\n",
    "# Convert 'time_to_deliver' to minutes\n",
    "data2['time_to_deliver'] = pd.to_timedelta(data2['time_to_deliver']).dt.total_seconds() / 60\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data2['weather_conditions'] = label_encoder.fit_transform(data2['weather_conditions'])"
   ],
   "id": "19dc33bd1b8a5152"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data2",
   "id": "f898bb2bb31c02d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # Step 5: Perform Exploratory Data Analysis\n",
    "---\n",
    "- Distribution Analysis:\n",
    "    1. How frequent do we get orders?\n",
    "    2. How Fast is our delivery time?\n",
    "    3. How far is our customers from their preferred restuarant?\n",
    "    4. How much do our customer order?\n",
    "    5. Where do mostly of the customer buy from?\n",
    "    \n",
    "- Relationship analysis:\n",
    "    6. What is the relationship between Weather Condition, traffic condition and order size?\n",
    "    7. What is the relationship between Weather Condition, traffic condition and delivery time?\n",
    "\n",
    "##### Note: We will not perform correlation analysis at this part as this will be done during the modeling part."
   ],
   "id": "d4b2922d9018a0ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 1. How frequent do we get orders?\n",
    "---\n",
    "- There is no need to graph since it is obvious that we get new orders every 5 minutes"
   ],
   "id": "a2fe8d308b2cdcbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pd.to_timedelta(data2.order_timestamp.diff())",
   "id": "fb71112fba9efdd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 2. How Fast is our delivery time?\n",
    "---\n",
    "Most of out drivers travel at 30 minutes to deliver an order."
   ],
   "id": "bf07b4a972a5e46f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "group_labels = ['Delivery']\n",
    "fig = ff.create_distplot([data2.time_to_deliver], group_labels)\n",
    "fig.show()"
   ],
   "id": "78d9b0a72bace5f5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 3. How far is our customers from their preferred restuarant?\n",
    "---\n",
    "Most of our customers are within the 3 kilometer radius from their preferred restaurant."
   ],
   "id": "b35d6b3dcd85f772"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "group_labels = ['Distance']\n",
    "fig = ff.create_distplot([data2.distance_km], group_labels)\n",
    "fig.show()"
   ],
   "id": "d8cf2e3b5f13bbaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 4. How much do our customer order?\n",
    "---\n",
    "- Our customers usually has an order size of between 2 to 3 products."
   ],
   "id": "21e6a7f8a3fa9b1d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "group_labels = ['Size']\n",
    "fig = ff.create_distplot([data2.order_size], group_labels)\n",
    "fig.show()"
   ],
   "id": "45602db02efa8ce3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 6. What is the relationship between Weather Condition, traffic condition and order size?\n",
    "---\n",
    "- Most orders are done during a cloudy day, and less orders are done on a clear day. This is obvious since people tend to buy on a fair weather, and drivers tend not to accept orders during rainy weathers.\n",
    "- Interestingy, orders are high during a cloudy weather and high traffic."
   ],
   "id": "3db3d178e988d6db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rel1 = data[[\"order_size\",\"weather_conditions\",\"traffic_conditions\"]]\n",
    "rel1[\"ave_order_size\"] = rel1.groupby([\"weather_conditions\",\"traffic_conditions\"])[\"order_size\"].transform(\"mean\")\n",
    "rel1 = rel1.drop(\"order_size\", axis=1)"
   ],
   "id": "34d143d9f55b20b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=rel1.ave_order_size,\n",
    "                   x=rel1.traffic_conditions,\n",
    "                   y=rel1.weather_conditions,\n",
    "                   hoverongaps = False))\n",
    "fig.show()"
   ],
   "id": "f2e3c1c1b69e746"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 7. What is the relationship between Weather Condition, traffic condition and delivery time?\n",
    "---\n",
    "- Orders are delayed mostly during a cloudy weather, followed by during rain, lastly during a foggy weather. This might be because people tend to panic to travel home during a cloudy weather to avoid the rain."
   ],
   "id": "c6d85f08c11d199a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "rel2_1 = data[[\"order_id\",\"weather_conditions\",\"traffic_conditions\"]]\n",
    "rel2_2 = data2[[\"order_id\",\"traffic_conditions\",\"time_to_deliver\"]]\n",
    "rel2 = pd.merge(rel2_1,rel2_2, how = \"right\", on = [\"order_id\",\"traffic_conditions\"])\n",
    "rel2[\"ave_time_to_deliver\"] = rel2.groupby([\"weather_conditions\",\"traffic_conditions\"])[\"time_to_deliver\"].transform(\"mean\")\n",
    "rel2 = rel2.drop(\"time_to_deliver\", axis=1)"
   ],
   "id": "e2bff21f60a7535a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = go.Figure(data=go.Heatmap(\n",
    "                   z=rel2.ave_time_to_deliver,\n",
    "                   x=rel2.traffic_conditions,\n",
    "                   y=rel2.weather_conditions,\n",
    "                   hoverongaps = False))\n",
    "fig.show()"
   ],
   "id": "32d03c8d09d5d41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Baseline Model: Linear Regression\n",
    "---\n",
    "For the baseline model the standard method is linear regression due to its simplicity, ease of interpretation, and ease of implementation."
   ],
   "id": "4e35ec104c0835c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df_reg = data2.drop([\"order_id\",\"order_timestamp\",\"delivery_timestamp\"], axis=1)\n",
    "df_reg"
   ],
   "id": "aaf51381cca4d7ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Correlation Analysis\n",
    "---\n",
    "\"Not all models are created equal\"\n",
    "\n",
    "For linear models it is necessary to remove highly correlated variables due to the following reasons:\n",
    "\n",
    "1. Unstable Estimates of Coefficients:\n",
    "\n",
    "    - When predictor variables are highly correlated, it becomes difficult for the linear model to determine the individual effect of each predictor on the response variable. This leads to unstable estimates of the regression coefficients, which can vary widely with small changes in the data.\n",
    "\n",
    "2. Inflated Standard Errors:\n",
    "\n",
    "    - Multicollinearity increases the standard errors of the estimated coefficients, making the model less reliable. This means that even if a variable is actually relevant, the model might fail to identify it as statistically significant due to the inflated standard error.\n",
    "\n",
    "\n",
    "3. Difficulty in Interpreting Coefficients:\n",
    "\n",
    "     - In a linear model, coefficients represent the change in the response variable for a one-unit change in the predictor, holding all other variables constant. When variables are highly correlated, this interpretation becomes meaningless because it’s not possible to change one predictor without changing the other.\n",
    "\n",
    "4. Reduced Predictive Power:\n",
    "\n",
    "    - Although multicollinearity does not always impact the predictive ability of the model on the training data, it can negatively affect the model's generalization to new data. This is because the model relies on relationships that may not hold outside the training dataset.\n",
    "    \n",
    "Correlation coefficients between 0.7 to 1.0 are considered highly correlated\n",
    "\n",
    "#### Analysis\n",
    "---\n",
    "- Weather conditions and traffic are strongly correlated to the dependent variable, time to deliver. This indicates that these two variables have a strong linear relationship with the delivery time and are might therefore good predictors.\n",
    "\n",
    "- Restaurant popularity is strongly, but negatively, correlated with traffic conditions, indicating that these two predictors convey the same information. For this experiment, restaurant popularity will be removed."
   ],
   "id": "2c5be2a809f2c401"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# calculate the correlation matrix on the numeric columns\n",
    "corr = df_reg.select_dtypes('number').corr()\n",
    "\n",
    "cmap = sns.diverging_palette(5, 250, as_cmap=True)\n",
    "\n",
    "def magnify():\n",
    "    return [dict(selector=\"th\",\n",
    "                 props=[(\"font-size\", \"7pt\")]),\n",
    "            dict(selector=\"td\",\n",
    "                 props=[('padding', \"0em 0em\")]),\n",
    "            dict(selector=\"th:hover\",\n",
    "                 props=[(\"font-size\", \"12pt\")]),\n",
    "            dict(selector=\"tr:hover td:hover\",\n",
    "                 props=[('max-width', '200px'),\n",
    "                        ('font-size', '12pt')])\n",
    "]\n",
    "\n",
    "corr.style.background_gradient(cmap, axis=1)\\\n",
    "    .format(precision=3)\\\n",
    "    .set_properties(**{'max-width': '80px', 'font-size': '10pt'})\\\n",
    "    .set_caption(\"Correlation Coefficient\")\\\n",
    "    .set_table_styles(magnify())"
   ],
   "id": "bb144de7eb7134ae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Separate features and target variable\n",
    "X_reg1 = df_reg.drop(columns=['time_to_deliver','restaurant_popularity'])\n",
    "y_reg1 = df_reg['time_to_deliver']\n",
    "# Split the data\n",
    "X_train_reg1, X_test_reg1, y_train_reg1, y_test_reg1 = train_test_split(X_reg1, y_reg1, test_size=0.2, random_state=42)"
   ],
   "id": "dbccdc95764c436d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define the dependent and independent variables\n",
    "X = X_reg1[['distance_km','weather_conditions', 'traffic_conditions']]\n",
    "y = y_reg1\n",
    "\n",
    "# Add a constant term for the intercept\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "print(\"\\nFinal model summary:\")\n",
    "print(model.summary())"
   ],
   "id": "9c1a70913e5ea209"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Feature Selection: Bacward Elimination\n",
    "---\n",
    "- The linear model starts with the entire variables and individually remove each variables based on a level of significance. In this experiment the order size does not significantly contribute to the prediction model.\n",
    "- distance, weather condition, and traffic condition have statistically signicant levels indicating that these are good predictors of the outcome.\n",
    "- This experiment validates the correlation analysis above, as can be seen that the order size is only mildly correlated to the outcome with a value of 0.513.\n",
    "- The regression line is almost perfectly linear with an adjusted $R^2$ of 0.944.\n",
    "- However, the number of sample is not enough to check for the impact of kurtosis (the shape of the data distribution), since linear regression models are heavily dependent on the normality assumption."
   ],
   "id": "e0ca7fd5de327f9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Backward elimination function\n",
    "def backward_elimination(X, y, significance_level=0.05):\n",
    "    features = list(X.columns)\n",
    "    while len(features) > 0:\n",
    "        X_with_constant = add_constant(X[features])\n",
    "        model = sm.OLS(y, X_with_constant).fit()\n",
    "        pvalues = model.pvalues.iloc[1:]  # Skip the intercept's p-value\n",
    "        max_pval = pvalues.max()\n",
    "\n",
    "        if max_pval > significance_level:\n",
    "            # Remove the feature with the highest p-value\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            features.remove(worst_feature)\n",
    "            print(f\"Removed feature: {worst_feature}, p-value: {max_pval}\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Fit the final model with remaining features\n",
    "    final_model = sm.OLS(y, add_constant(X[features])).fit()\n",
    "    return final_model, features\n",
    "\n",
    "# Run backward elimination on the training data\n",
    "final_model, selected_features = backward_elimination(X_train_reg1, y_train_reg1)\n",
    "\n",
    "# Print the summary of the final model\n",
    "print(\"\\nFinal model summary:\")\n",
    "print(final_model.summary())\n",
    "\n",
    "# Print the remaining features after backward elimination\n",
    "print(\"\\nSelected features after backward elimination:\")\n",
    "print(selected_features)"
   ],
   "id": "f1b4e5b59e24b372"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. K-Fold Validation\n",
    "---\n",
    "- To create a more robust overview of the model given the different variability of the data a k-fold corss validation is conducted, root mean squared error is used as the performance metric."
   ],
   "id": "270271c2efb4867c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Separate features and target variable\n",
    "X = df_reg[['distance_km','weather_conditions', 'traffic_conditions']]\n",
    "y = df_reg['time_to_deliver']\n",
    "\n",
    "# Initialize KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Store the MSE values for each fold\n",
    "rmse_values = []\n",
    "\n",
    "# Perform the K-Fold Cross-Validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Fit the model on the training set\n",
    "    model = sm.OLS(y_train, sm.add_constant(X_train)).fit()\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(sm.add_constant(X_test))\n",
    "    \n",
    "    # Calculate the mean squared error\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "    rmse_values.append(rmse)\n",
    "\n",
    "# Output the mean MSE over the folds\n",
    "print(f\"Mean RMSE: {np.mean(rmse_values)} minutes\")"
   ],
   "id": "3838c501d1934576"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Proposed Model: XGBOOST\n",
    "---\n",
    "\n",
    "#### Why XGBOOST?\n",
    "---\n",
    "- XGBoost (Extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm, which is designed for speed and performance. It builds an ensemble of decision trees in a sequential manner to improve predictive accuracy, and each tree tries to correct the errors of the previous ones. Key features of XGBoost include:\n",
    "\n",
    "- Regularization: It uses both L1 (Lasso) and L2 (Ridge) regularization to prevent overfitting, making it robust to noisy data.\n",
    "\n",
    "- Tree Pruning: XGBoost employs a technique called \"max depth\" pruning, where it starts with a full tree and prunes unnecessary branches.\n",
    "\n",
    "- Handling Missing Values: It can natively handle missing values in the dataset by learning which path to take based on the observed values.\n",
    "\n",
    "- Parallelization: The algorithm supports parallel computation, making it faster than other boosting methods.\n",
    "\n",
    "- Custom Loss Functions: XGBoost allows for user-defined objective functions for specific use cases.\n",
    "\n",
    "\n",
    "#### Insensitivity to Correlated Variables\n",
    "---\n",
    "- XGBoost, like other tree-based algorithms, is generally less sensitive to multicollinearity (correlated variables) compared to linear models. The decision tree structure allows it to handle correlated features without significantly degrading performance."
   ],
   "id": "f343c60211d2b2a3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Separate features and target variable\n",
    "df = df_reg.copy()\n",
    "X = df.drop(columns=['time_to_deliver'])\n",
    "y = df['time_to_deliver']\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "id": "79a06c0ff215aa18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train.columns",
   "id": "2d61c97a1afa7665"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 1: Fine tuning the model\n",
    "---\n",
    "- XGBoost tend to have multiple parameters to tune, to find the optimal parameters Parzan Tree is used, with RMSE as the performance metric."
   ],
   "id": "7ee0c0348197a4d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0)\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = root_mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    return rmse"
   ],
   "id": "8defa374f0b4ccb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a study to minimize the RMSE\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # You can increase n_trials for a more exhaustive search\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study.best_params)"
   ],
   "id": "5730533df6eac07a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 2: Use the optimal parameters to train the XGBoost model\n",
    "---\n",
    "- Check the optimal RMSE after training."
   ],
   "id": "9f9b2156ab053a79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the best hyperparameters to train the final model\n",
    "best_params = study.best_params\n",
    "best_model = xgb.XGBRegressor(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = root_mean_squared_error(y_test, y_pred)\n",
    "print(f'Optimized RMSE: {rmse:.2f} minutes')"
   ],
   "id": "ee428814cc0129ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 3: Checking for Feature Importance\n",
    "---\n",
    "\n",
    "1. What is Gain in XGBoost?\n",
    "Gain measures the improvement in the model's objective function (e.g., error reduction or information gain) when a feature is used to split the data at a node in the decision tree. It represents how much a feature contributes to reducing the prediction error. XGBoost builds trees iteratively, and at each step, it looks for the best split to reduce the objective function. The \"gain\" is used to evaluate which feature should be split at each point in the tree.\n",
    "\n",
    "2. How is Gain Calculated?\n",
    "\n",
    "- Objective Function: In XGBoost, the objective function consists of a loss function (such as squared error for regression or log-loss for classification) and a regularization term to prevent overfitting.\n",
    "\n",
    "- Gain Calculation: Gain is calculated based on the difference between the loss function before and after splitting on a particular feature. At each possible split point, XGBoost evaluates how the split affects the error. It checks the error reduction achieved by splitting the data into two sub-groups and sums the errors for both groups.\n",
    "\n",
    "For a feature, $f$, gain is calculated as:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Information Score} = Score_{\\text{left child}} + Score_{\\text{right child}} − Score_{\\text{Parent}}\n",
    "\\end{align}\n",
    " \n",
    "where:\n",
    "\n",
    "    - Score represents the contribution of a node to the overall model performance, based on the loss function.\n",
    "    - The left child and right child refer to the two nodes created after the split.\n",
    "\n",
    "- The goal is to maximize this gain at each node when building the decision tree.\n",
    "\n",
    "- Feature Importance: Once the decision trees are built, the importance of a feature is derived by summing up the gain for all splits that use that feature across all trees in the model. Features with higher cumulative gains are considered more important since they contribute more to reducing the prediction error.\n",
    "\n",
    "3 . Why Use Gain for Feature Importance?\n",
    "\n",
    "- Interpretability: Gain provides a direct measure of how useful a feature is for improving model accuracy. A higher gain indicates that a feature plays a more significant role in model performance.\n",
    "\n",
    "- Ranking Features: It allows us to rank features based on their contribution to the model, which is valuable for understanding which features are most influential in making predictions.\n",
    "\n",
    "4. Limitations:\n",
    "\n",
    "- Bias towards features with many possible values: Gain might give more importance to features with a high number of potential split points, like continuous variables, which can sometimes result in overestimating their importance.\n",
    "\n",
    "#### Interpreation\n",
    "---\n",
    "\n",
    "- The result showed that only distance and restaurant popularity has an impact on the XGBOOST regresstion model."
   ],
   "id": "63332f7a0e557be6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Plot feature importance\n",
    "# Convert feature importance to a DataFrame\n",
    "importance = best_model.get_booster().get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame(list(importance.items()), columns=['Feature', 'Importance'])\n",
    "\n",
    "# Plot using matplotlib\n",
    "plt.barh(importance_df['Feature'], importance_df['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance - Gain')\n",
    "plt.gca().invert_yaxis() # To display the most important feature at the top\n",
    "plt.show()"
   ],
   "id": "cbb8aaa2a98424bf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Step 4: Retraining the model with the important features",
   "id": "db342b799b17255e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "df2 = df.drop(['order_size','weather_conditions', 'traffic_conditions'], axis=1)",
   "id": "ca0108c9b16d7b99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Separate features and target variable\n",
    "X2 = df2.drop(columns=['time_to_deliver'])\n",
    "y2 = df2['time_to_deliver']\n",
    "# Split the data\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, test_size=0.2, random_state=42)"
   ],
   "id": "79a7c60c427df938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "X_train2.columns",
   "id": "79e01d27201fa0ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def objective2(trial):\n",
    "    # Define the hyperparameters to tune\n",
    "    param = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.3),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 5),\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0)\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    model = xgb.XGBRegressor(**param)\n",
    "    model.fit(X_train2, y_train2)\n",
    "\n",
    "    # Predict on the validation set\n",
    "    y_pred2 = model.predict(X_test2)\n",
    "\n",
    "    # Calculate the RMSE\n",
    "    rmse = root_mean_squared_error(y_test2, y_pred2)\n",
    "\n",
    "    return rmse"
   ],
   "id": "db96dd5a00e3dcee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Create a study to minimize the RMSE\n",
    "study2 = optuna.create_study(direction='minimize')\n",
    "study2.optimize(objective2, n_trials=50)  # You can increase n_trials for a more exhaustive search\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\", study2.best_params)"
   ],
   "id": "c7bbb1e1b9922b3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Use the best hyperparameters to train the final model\n",
    "best_params2 = study2.best_params\n",
    "best_model2 = xgb.XGBRegressor(**best_params2)\n",
    "best_model2.fit(X_train2, y_train2)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred2 = best_model2.predict(X_test2)\n",
    "rmse = root_mean_squared_error(y_test2, y_pred2)\n",
    "print(f'Optimized RMSE: {rmse:.2f} minutes')"
   ],
   "id": "1732e0d4b9879205"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Step 5: Performing K-Fold Validation\n",
    "---"
   ],
   "id": "1690bb418b008d76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Perform 5-fold cross-validation\n",
    "k = 5\n",
    "scores = cross_val_score(best_model, X, y, cv=k, scoring='neg_root_mean_squared_error')\n",
    "\n",
    "# Convert scores to positive and print the average RMSE\n",
    "rmse_scores = -scores\n",
    "print(f'RMSE scores for each fold: {rmse_scores}')\n",
    "print(f'Average RMSE: {np.mean(rmse_scores):.2f} minutes')"
   ],
   "id": "8bc0fb557cdf17d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Recommendations\n",
    "---\n",
    "\n",
    "1. Prepare more on-call drivers during cloudy weather.\n",
    "2. Have more call agents during cloudy weather to recieve complaints.\n",
    "3. Since app usage is high during cloudy weather, provide higher storing and computing capacity during this season.\n",
    "4. More data to achieve a more accurate model."
   ],
   "id": "a30033cca081f8b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Conclusion",
   "id": "e1150283f936aa5f"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
